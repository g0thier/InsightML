{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InsightML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_flavor as pf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def about_my_csv(filename):\n",
    "    # Ouvrir le fichier en mode lecture et détecter le format de délimitation de colonne\n",
    "    with open(filename, 'r', newline='') as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
    "    return dialect\n",
    "\n",
    "def about_my_xls(filename):\n",
    "    # Lire le fichier Excel en utilisant le module openpyxl et sans en-têtes de colonne\n",
    "    df_temp = pd.read_excel(filename, engine='openpyxl', header=None)\n",
    "    # Détecter le format de délimitation de colonne à l'aide de la fonction csv.Sniffer().sniff()\n",
    "    dialect = csv.Sniffer().sniff(df_temp.to_csv(index=False, header=False))\n",
    "    return dialect\n",
    "\n",
    "def about_my_json(filename):\n",
    "    # Lire le fichier Json pour récuperer la donnée\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    # Détecter le format du fichier JSON\n",
    "    try:\n",
    "        json.loads(data)\n",
    "        orient = 'records'\n",
    "    except ValueError:\n",
    "        orient = 'columns'\n",
    "    return orient\n",
    "\n",
    "def about_my_h5(filename):\n",
    "    # Lire les clés disponibles dans le fichier HDF5\n",
    "    with pd.HDFStore(filename, mode='r') as store:\n",
    "        keys = store.keys()\n",
    "\n",
    "    # Trouver la clé correspondant au plus grand ensemble de données\n",
    "    max_size = 0\n",
    "    max_key = None\n",
    "    for key in keys:\n",
    "        size = pd.read_hdf(filename, key=key, stop=0).memory_usage(index=True, deep=True).sum()\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            max_key = key\n",
    "    return max_key\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_file = 'chemin/vers/fichier.csv'\n",
    "\n",
    "extention = path_to_file.split('.')[-1]\n",
    "\n",
    "if extention == 'csv':\n",
    "    # Infos format about my csv.\n",
    "    my_dialect = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_csv(path_to_file, dialect=my_dialect)\n",
    "\n",
    "elif extention in ['xls', 'xlsx','xlsm','xlsb']:\n",
    "    # Infos format about my excel.\n",
    "    my_dialect = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_excel(path_to_file, engine='openpyxl', delimiter=my_dialect.delimiter)\n",
    "\n",
    "elif extention == 'json':\n",
    "    # Infos format about my json.\n",
    "    my_orient = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_json(path_to_file, orient=my_orient)\n",
    "\n",
    "elif extention == 'h5':\n",
    "    # Infos format about my json.\n",
    "    my_key = about_my_h5(path_to_file)\n",
    "    # Make dataframe\n",
    "    df = pd.read_hdf(path_to_file, key=my_key, mode='r')\n",
    "\n",
    "else: \n",
    "    print('format non pris en charge')\n",
    "    print('Option à venir : SQL, Parquet, Feather, Pickle, HTML, XML')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du pourcentage de lignes vides pour chaque colonne\n",
    "pourcentage_lignes_vides = df.isna().sum() / len(df)\n",
    "\n",
    "# Sélection des colonnes à conserver (celles ayant moins de 30% de lignes vides)\n",
    "colonnes_a_conserver = pourcentage_lignes_vides[pourcentage_lignes_vides <= 0.3].index\n",
    "\n",
    "# Création d'un nouveau DataFrame ne contenant que les colonnes à conserver\n",
    "df = df[colonnes_a_conserver]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supprime les lignes avec des données sporatiques\n",
    "# Calculer le pourcentage de valeurs manquantes dans chaque ligne\n",
    "missing_pct = df.isnull().sum(axis=1) / df.shape[1]\n",
    "# Calculer le seuil de tolérance en nombre de colonnes sans valeurs\n",
    "threshold = 0.15 * df.shape[1]\n",
    "# Supprimer les lignes dont le nombre de colonnes sans valeurs est supérieur au seuil de tolérance\n",
    "df = df.dropna(thresh=df.shape[1]-threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la méthode 'detect_date' pour détecter les colonnes de texte contenant des dates\n",
    "@pf.register_dataframe_method\n",
    "def detect_date(df):\n",
    "    for col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "        except ValueError:\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "# Détecter et convertir les colonnes de texte en dates\n",
    "df = df.detect_date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes de type datetime\n",
    "date_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "\n",
    "# Vérifier s'il y a au moins deux colonnes de date\n",
    "if len(date_cols) < 2:\n",
    "    raise ValueError('Il n\\'y a pas assez de colonnes de date pour faire des différences de temps.')\n",
    "else:\n",
    "    # Normaliser les dates sur UTC\n",
    "    for date_col in date_cols:\n",
    "        df[date_col] = df[date_col].dt.tz_convert('Etc/UTC')\n",
    "\n",
    "    # Créer toutes les combinaisons de colonnes de date\n",
    "    list_date_cols = list(itertools.combinations(date_cols, 2))\n",
    "\n",
    "    # Calculer la différence de temps entre chaque paire de colonnes de date\n",
    "    for paire_date in list_date_cols:\n",
    "        first_date = paire_date[0]\n",
    "        second_date = paire_date[1]\n",
    "\n",
    "        df[f'{first_date}_x_{second_date}'] = (df[second_date] - df[first_date]).dt.total_seconds()\n",
    "\n",
    "\n",
    "# Extraire : Year, Day, WeekDay, Hour, Minute, Second, Microsecond\n",
    "for date_col in date_cols:\n",
    "    # Ajouter les colonnes pour chaque attribut de date\n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_day_of_year'] = df[date_col].dt.dayofyear\n",
    "    df[f'{date_col}_day_of_week'] = df[date_col].dt.dayofweek\n",
    "    df[f'{date_col}_hour'] = df[date_col].dt.hour\n",
    "    df[f'{date_col}_minute'] = df[date_col].dt.minute\n",
    "    df[f'{date_col}_second'] = df[date_col].dt.second\n",
    "    df[f'{date_col}_microsecond'] = df[date_col].dt.microsecond\n",
    "\n",
    "    df.drop(date_col, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "numeric_features = df.select_dtypes([np.number]).columns\n",
    "\n",
    "for numerical in numeric_features:\n",
    "    # Garde uniquement les valeurs qui sont dans l'intervalle [moyenne-2*ecart_type, moyenne+2*ecart_type]\n",
    "    to_keep = (df[numerical] > df[numerical].mean() - 2*df[numerical].std()) & (df[numerical] < df[numerical].mean() + 2*df[numerical].std())\n",
    "    df = df.loc[to_keep,:]\n",
    "\n",
    "    # Transforme en catégoriel quand moins de 8 valeurs différentes.\n",
    "    if df[numerical].value_counts() <= 8:\n",
    "        df[numerical] = df[numerical].astype(str)\n",
    "\n",
    "    # Supression valeurs manquantes au dessus de 10 %\n",
    "    pourcentage_valeur_manquante = 100*df[numerical].isnull().sum()/len(df)\n",
    "    if pourcentage_valeur_manquante >= 10:\n",
    "        df.drop(numerical, axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categoriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df.select_dtypes(\"object\").columns\n",
    "\n",
    "for categorical in categorical_features:\n",
    "    # Remplacement des valeurs nulles par la valeur \"manquante\"\n",
    "    df[categorical] = df[categorical].fillna('manquante')\n",
    "\n",
    "    ## Remplace les valeurs sporatique par la valeur \"autre\"\n",
    "    # Calcul du pourcentage de chaque valeur dans la colonne\n",
    "    counts = df[categorical].value_counts(normalize=True)\n",
    "    # Sélection des valeurs qui représentent moins de 10% de la colonne\n",
    "    mask = (counts < 0.1)\n",
    "    # Remplacement des valeurs sélectionnées par la valeur \"autre\"\n",
    "    df[categorical] = df[categorical].replace(counts[mask].index.tolist(), 'autre')\n",
    "\n",
    "    # Suppression des colonnes contenant une seule valeur\n",
    "    if df[categorical].nunique() == 1:\n",
    "        df.drop(categorical, axis=1, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Par la suite\n",
    "* Détecter format carte bancaire, indicatif telephone...\n",
    "* Convertir adresse en format lat.nlong\n",
    "* Convertir les listes de mots en catégories quand c'est possible.\n",
    "* ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Séparation en variables explicative et cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = df.columns[-1]\n",
    "\n",
    "Y = df[:][target_name]\n",
    "X = df.drop(columns=[target_name])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suppr colonnes ultra corrélées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = X.corr()\n",
    "\n",
    "high_corr_list = []\n",
    "cols = corr.columns\n",
    "\n",
    "for j in cols:\n",
    "    for i, item in corr[j].iteritems():\n",
    "        if (i!=j) and abs(item) > 0.9:\n",
    "            high_corr_list.append((i,j))\n",
    "high_corr_list\n",
    "\n",
    "no_keep = [high_corr_list[i][0] for i in range(len(high_corr_list)) if i%2 == 0]\n",
    "\n",
    "columns_to_keep = [c for c in X.columns if c not in no_keep]\n",
    "\n",
    "X_clean = X.loc[:, columns_to_keep]\n",
    "X_clean.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression \n",
    "\n",
    "#### Séparation variable explicative et target \n",
    "X\n",
    "Y\n",
    "\n",
    "#### Séparation en Entrainement & Test\n",
    "X_train X_test \n",
    "!!! Stratify si Y valeur catégoriel \n",
    "\n",
    "#### Régler déséquilibre dataset\n",
    "pour une différence supérieur à 10%, \n",
    "à noté par la suite qu'une meilleur approche est de faire attention :\n",
    "à la précision, le rappel, la F1-score et l'aire sous la courbe ROC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rUs=RandomUnderSampler()\n",
    "X_ru, y_ru = rUs.fit_resample(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "numeric_features, categorical_features, preprocessor, StandardScaler....\n",
    "\n",
    "#### Training Model \n",
    "Logistic Regression\n",
    "Linear Regression\n",
    "Random Forest Classifier\n",
    "Random Forest Classifier \n",
    "\n",
    "### Result \n",
    "\n",
    "#### Qualitatif (Classification)\n",
    "Accuracy \n",
    "\n",
    "#### Quantitatif (Numérique)\n",
    "R2\n",
    "\n",
    "#### Grid search : Random Forest Classifier\n",
    "Accuracy\n",
    "F1\n",
    "\n",
    "#### Grid search : Random Forest Regressor \n",
    "print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
