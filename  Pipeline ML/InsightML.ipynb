{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InsightML"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import itertools\n",
    "import json\n",
    "import pandas as pd "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def about_my_csv(filename):\n",
    "    # Ouvrir le fichier en mode lecture et détecter le format de délimitation de colonne\n",
    "    with open(filename, 'r', newline='') as csvfile:\n",
    "        dialect = csv.Sniffer().sniff(csvfile.read(1024))\n",
    "    return dialect\n",
    "\n",
    "def about_my_xls(filename):\n",
    "    # Lire le fichier Excel en utilisant le module openpyxl et sans en-têtes de colonne\n",
    "    df_temp = pd.read_excel(filename, engine='openpyxl', header=None)\n",
    "    # Détecter le format de délimitation de colonne à l'aide de la fonction csv.Sniffer().sniff()\n",
    "    dialect = csv.Sniffer().sniff(df_temp.to_csv(index=False, header=False))\n",
    "    return dialect\n",
    "\n",
    "def about_my_json(filename):\n",
    "    # Lire le fichier Json pour récuperer la donnée\n",
    "    with open(filename, 'r') as f:\n",
    "        data = f.read()\n",
    "    # Détecter le format du fichier JSON\n",
    "    try:\n",
    "        json.loads(data)\n",
    "        orient = 'records'\n",
    "    except ValueError:\n",
    "        orient = 'columns'\n",
    "    return orient\n",
    "\n",
    "def about_my_h5(filename):\n",
    "    # Lire les clés disponibles dans le fichier HDF5\n",
    "    with pd.HDFStore(filename, mode='r') as store:\n",
    "        keys = store.keys()\n",
    "\n",
    "    # Trouver la clé correspondant au plus grand ensemble de données\n",
    "    max_size = 0\n",
    "    max_key = None\n",
    "    for key in keys:\n",
    "        size = pd.read_hdf(filename, key=key, stop=0).memory_usage(index=True, deep=True).sum()\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            max_key = key\n",
    "    return max_key\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_to_file = 'chemin/vers/fichier.csv'\n",
    "\n",
    "extention = path_to_file.split('.')[-1]\n",
    "\n",
    "if extention == 'csv':\n",
    "    # Infos format about my csv.\n",
    "    my_dialect = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_csv(path_to_file, dialect=my_dialect)\n",
    "\n",
    "elif extention in ['xls', 'xlsx','xlsm','xlsb']:\n",
    "    # Infos format about my excel.\n",
    "    my_dialect = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_excel(path_to_file, engine='openpyxl', delimiter=my_dialect.delimiter)\n",
    "\n",
    "elif extention == 'json':\n",
    "    # Infos format about my json.\n",
    "    my_orient = about_my_csv(path_to_file)\n",
    "    # Make dataframe.\n",
    "    df = pd.read_json(path_to_file, orient=my_orient)\n",
    "\n",
    "elif extention == 'h5':\n",
    "    # Infos format about my json.\n",
    "    my_key = about_my_h5(path_to_file)\n",
    "    # Make dataframe\n",
    "    df = pd.read_hdf(path_to_file, key=my_key, mode='r')\n",
    "\n",
    "else: \n",
    "    print('format non pris en charge')\n",
    "    print('Option à venir : SQL, Parquet, Feather, Pickle, HTML, XML')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp Colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du pourcentage de lignes vides pour chaque colonne\n",
    "pourcentage_lignes_vides = df.isna().sum() / len(df)\n",
    "\n",
    "# Sélection des colonnes à conserver (celles ayant moins de 30% de lignes vides)\n",
    "colonnes_a_conserver = pourcentage_lignes_vides[pourcentage_lignes_vides <= 0.3].index\n",
    "\n",
    "# Création d'un nouveau DataFrame ne contenant que les colonnes à conserver\n",
    "df = df[colonnes_a_conserver]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supp lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer le pourcentage de valeurs manquantes dans chaque ligne\n",
    "missing_pct = df.isnull().sum(axis=1) / df.shape[1]\n",
    "\n",
    "# Calculer le seuil de tolérance en nombre de colonnes sans valeurs\n",
    "threshold = 0.15 * df.shape[1]\n",
    "\n",
    "# Supprimer les lignes dont le nombre de colonnes sans valeurs est supérieur au seuil de tolérance\n",
    "df = df.dropna(thresh=df.shape[1]-threshold)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes de type datetime\n",
    "date_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "\n",
    "# Vérifier s'il y a au moins deux colonnes de date\n",
    "if len(date_cols) < 2:\n",
    "    raise ValueError('Il n\\'y a pas assez de colonnes de date pour faire des différences de temps.')\n",
    "else:\n",
    "    # Normaliser les dates sur UTC\n",
    "    for date_col in date_cols:\n",
    "        df[date_col] = df[date_col].dt.tz_convert('Etc/UTC')\n",
    "\n",
    "    # Créer toutes les combinaisons de colonnes de date\n",
    "    list_date_cols = list(itertools.combinations(date_cols, 2))\n",
    "\n",
    "    # Calculer la différence de temps entre chaque paire de colonnes de date\n",
    "    for paire_date in list_date_cols:\n",
    "        first_date = paire_date[0]\n",
    "        second_date = paire_date[1]\n",
    "\n",
    "        df[f'{first_date}_x_{second_date}'] = (df[second_date] - df[first_date]).dt.total_seconds()\n",
    "\n",
    "# Convertion en attribut.\n",
    "for date_col in date_cols:\n",
    "    # Ajouter les colonnes pour chaque attribut de date\n",
    "    df[f'{date_col}_year'] = df[date_col].dt.year\n",
    "    df[f'{date_col}_day_of_year'] = df[date_col].dt.dayofyear\n",
    "    df[f'{date_col}_day_of_week'] = df[date_col].dt.dayofweek\n",
    "    df[f'{date_col}_hour'] = df[date_col].dt.hour\n",
    "    df[f'{date_col}_minute'] = df[date_col].dt.minute\n",
    "    df[f'{date_col}_second'] = df[date_col].dt.second\n",
    "    df[f'{date_col}_microsecond'] = df[date_col].dt.microsecond\n",
    "\n",
    "    df.drop(date_col, axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Extraire : Year, Month, Day, Hour, Minute, Second, Microsecond"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DateTime\n",
    "Convertir en UTC pour normaliser la date, \n",
    "Puis extraire : Year, Month, Day, Hour, Minute, Second, Microsecond.\n",
    "\n",
    "#### Int, Float\n",
    "Gérer comme des données catégoriels si moins de 8 valeurs différentes.\n",
    "Suppressions valeurs abérantes et loin de l'écart type. \n",
    "Supprimer les colonnes contenant trop d'informations manquantes. (% 10)\n",
    "\n",
    "\n",
    "#### Bool, Str \n",
    "Gérer comme des données numériques si que des chiffres.\n",
    "Remplacer les nulls par une valeur \"Manquante\"\n",
    "Réduire les termes à moins de 10 % en catégorie \"Autre\". \n",
    "Si la catégorie \"Autre\" fait moins de 5 % ou est l'unique catégorie restante, supprime la colonne. \n",
    "\n",
    "#### Lignes \n",
    "supression valeur aberante\n",
    "supprimer variables explicative ultra corrélées à + 90 %\n",
    "\n",
    "#### Par la suite\n",
    "Détecter format carte bancaire, etc. \n",
    "Convertir adresse en format lat.nlong\n",
    "Convertir les listes en catégories quand c'est possible.\n",
    "Differences entre les dates.\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression \n",
    "\n",
    "#### Séparation variable explicative et target \n",
    "X\n",
    "Y\n",
    "\n",
    "#### Séparation en Entrainement & Test\n",
    "X_train X_test \n",
    "!!! Stratify si Y valeur catégoriel \n",
    "\n",
    "#### Régler déséquilibre dataset\n",
    "pour une différence supérieur à 10%, \n",
    "à noté par la suite qu'une meilleur approche est de faire attention :\n",
    "à la précision, le rappel, la F1-score et l'aire sous la courbe ROC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "rUs=RandomUnderSampler()\n",
    "X_ru, y_ru = rUs.fit_resample(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline \n",
    "numeric_features, categorical_features, preprocessor, StandardScaler....\n",
    "\n",
    "#### Training Model \n",
    "Logistic Regression\n",
    "Linear Regression\n",
    "Random Forest Classifier\n",
    "Random Forest Classifier \n",
    "\n",
    "### Result \n",
    "\n",
    "#### Qualitatif (Classification)\n",
    "Accuracy \n",
    "\n",
    "#### Quantitatif (Numérique)\n",
    "R2\n",
    "\n",
    "#### Grid search : Random Forest Classifier\n",
    "Accuracy\n",
    "F1\n",
    "\n",
    "#### Grid search : Random Forest Regressor \n",
    "print('MAE: ', mean_absolute_error(y_test, y_pred))\n",
    "print('MSE: ', mean_squared_error(y_test, y_pred)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
